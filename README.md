# Brain Age CNN - Deep Learning for Brain Age Prediction

## Overview

This project implements a deep learning pipeline for predicting brain age from medical imaging data using a fine-tuned DBN-VGG16 convolutional neural network. The system is designed for robust experimentation with comprehensive configuration management, cross-validation, and hyperparameter optimization.

## ğŸ¯ Key Features

- **Configuration-Driven Training**: All experiments managed through JSON/YAML configuration files
- **Flexible Data Loading**: CSV-based image loading with support for predefined train/val/test splits
- **Model Finetuning**: Intelligent layer-wise finetuning of pre-trained DBN-VGG16 architecture
- **Cross-Validation**: K-fold cross-validation with comprehensive hyperparameter grid search
- **Production Ready**: Robust error handling, logging, and experiment tracking
- **HiPerGator Compatible**: Optimized for University of Florida's HiPerGator cluster

## ğŸ—ï¸ System Architecture

### Core Components

```
Brain Age CNN Pipeline
â”œâ”€â”€ Configuration System (config.py)
â”‚   â”œâ”€â”€ DataConfig: Image paths, preprocessing, splits
â”‚   â”œâ”€â”€ ModelConfig: Architecture, finetuning settings
â”‚   â”œâ”€â”€ TrainingConfig: Optimization, callbacks, metrics
â”‚   â””â”€â”€ GridSearchConfig: Cross-validation, hyperparameters
â”‚
â”œâ”€â”€ Data Pipeline (data_loader_csv.py)
â”‚   â”œâ”€â”€ CSV-based image loading
â”‚   â”œâ”€â”€ Automatic train/val/test splitting
â”‚   â”œâ”€â”€ Image preprocessing and augmentation
â”‚   â””â”€â”€ Both PyTorch and Keras compatibility
â”‚
â”œâ”€â”€ Training Pipeline
â”‚   â”œâ”€â”€ Standard Training (train_with_config.py)
â”‚   â”œâ”€â”€ Cross-Validation Grid Search (train_cv_gridsearch_config.py)
â”‚   â””â”€â”€ Model Architecture (DBN-VGG16 finetuning)
â”‚
â””â”€â”€ Inference & Analysis
    â”œâ”€â”€ Model Inference (inference.py)
    â””â”€â”€ Results Analysis (analyze_cv_results.py)
```

### Model Architecture

The system employs a sophisticated finetuning strategy:

1. **Base Model**: Pre-trained DBN-VGG16 (Deep Belief Network + VGG16)
2. **Finetuning Strategy**: 
   - Last 3 dense layers: `dense_3`, `dropout_2`, `dense_4`
   - Optional VGG16 backbone unfreezing (configurable number of blocks)
3. **Output**: Single regression value (predicted age in years)
4. **Loss Function**: Mean Squared Error (MSE)
5. **Evaluation Metric**: Mean Absolute Error (MAE)

## ğŸ“ Project Structure

```
brain_age_cnn/
â”œâ”€â”€ ğŸ“ Core Python Files
â”‚   â”œâ”€â”€ config.py                      # Configuration management system
â”‚   â”œâ”€â”€ data_loader_csv.py             # CSV-based data loading pipeline
â”‚   â”œâ”€â”€ train_with_config.py           # Main training script
â”‚   â”œâ”€â”€ train_cv_gridsearch_config.py  # Cross-validation grid search
â”‚   â”œâ”€â”€ inference.py                   # Model inference and evaluation
â”‚   â””â”€â”€ analyze_cv_results.py          # Results analysis and visualization
â”‚
â”œâ”€â”€ ğŸ“ Configuration Files
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ default_config.json        # Default configuration
â”‚   â”‚   â”œâ”€â”€ quick_test_config.json     # Fast testing (5 epochs)
â”‚   â”‚   â”œâ”€â”€ full_training_config.json  # Production training (100 epochs)
â”‚   â”‚   â””â”€â”€ custom_config.json         # Custom experiment settings
â”‚
â”œâ”€â”€ ğŸ“ Data & Models
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset_splits.csv         # Main dataset CSV (user-provided)
â”‚   â”‚   â””â”€â”€ dataset_splits_example.csv # Example CSV format
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ DBN_VGG16.h5              # Pre-trained base model
â”‚
â”œâ”€â”€ ğŸ“ Output Directories (auto-created)
â”‚   â”œâ”€â”€ experiments/                   # Training outputs
â”‚   â””â”€â”€ cv_results/                    # Cross-validation results
â”‚
â””â”€â”€ ğŸ“ Documentation
    â”œâ”€â”€ README.md                      # This file
    â”œâ”€â”€ README_config_system.md        # Configuration system guide
    â”œâ”€â”€ PROJECT_STRUCTURE.md           # Detailed project structure
    â””â”€â”€ requirements.txt               # Python dependencies
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository and navigate to project directory
cd brain_age_cnn

# Install dependencies
pip install -r requirements.txt

# Verify required files exist
ls model/DBN_VGG16.h5  # Pre-trained model
ls data/dataset_splits.csv  # Your dataset CSV
```

### 2. Prepare Your Data

Create `data/dataset_splits.csv` with your brain imaging data:

```csv
patient_id,image_name,age,split
0001,slice_001.jpg,45.2,train
0002,slice_002.jpg,52.7,validation
0003,slice_003.jpg,38.9,test
```

**Required Columns:**
- `image_name`: Filename (relative to image directory)
- `age`: Ground truth age in years
- `patient_id`: Patient identifier
- `split`: train/validation/test (optional - auto-generated if missing)

### 3. Run Training

#### Quick Test (5 epochs)
```bash
python train_with_config.py --preset quick_test
```

#### Full Training (100 epochs)
```bash
python train_with_config.py --preset full_training
```

#### Custom Configuration
```bash
python train_with_config.py --config configs/custom_config.json
```

#### Cross-Validation Grid Search
```bash
python train_cv_gridsearch_config.py --preset grid_search
```

### 4. Model Inference

```bash
# Single image prediction
python inference.py --model_path experiments/my_experiment_20250624_120000/best_model.h5 \
                   --image_path /path/to/image.jpg

# Batch processing
python inference.py --model_path experiments/my_experiment_20250624_120000/best_model.h5 \
                   --batch_dir /path/to/images/ \
                   --output_csv predictions.csv
```

## âš™ï¸ Configuration System

### Configuration Hierarchy

The configuration system uses nested dataclasses for organized parameter management:

```python
ExperimentConfig
â”œâ”€â”€ data: DataConfig
â”‚   â”œâ”€â”€ image_list_csv: "data/dataset_splits.csv"
â”‚   â”œâ”€â”€ image_base_dir: "/blue/cruzalmeida/pvaldeshernandez/slices_for_deepbrainnet_new"
â”‚   â”œâ”€â”€ validation_split: 0.2
â”‚   â”œâ”€â”€ image_size: (224, 224)
â”‚   â””â”€â”€ augmentation: false
â”œâ”€â”€ model: ModelConfig
â”‚   â”œâ”€â”€ base_model_path: "model/DBN_VGG16.h5"
â”‚   â”œâ”€â”€ finetune_layers: ["dense_3", "dropout_2", "dense_4"]
â”‚   â””â”€â”€ finetune_vgg_blocks: 1
â”œâ”€â”€ training: TrainingConfig
â”‚   â”œâ”€â”€ batch_size: 16
â”‚   â”œâ”€â”€ epochs: 50
â”‚   â”œâ”€â”€ learning_rate: 0.0001
â”‚   â””â”€â”€ early_stopping_patience: 10
â””â”€â”€ grid_search: GridSearchConfig
    â”œâ”€â”€ n_folds: 3
    â”œâ”€â”€ batch_factors: [0.5, 1.0, 2.0]
    â””â”€â”€ learning_rates: [1e-6, 1e-5, 1e-4, 1e-3]
```

### Creating Custom Configurations

```python
from config import ExperimentConfig

# Create custom configuration
config = ExperimentConfig(
    name="my_experiment",
    description="Custom brain age prediction experiment"
)

# Modify specific parameters
config.training.batch_size = 32
config.training.epochs = 75
config.training.learning_rate = 0.00005
config.model.finetune_vgg_blocks = 2

# Save configuration
config.save("configs/my_experiment.json")
```

### Preset Configurations

Three preset configurations are available:

1. **Quick Test** (`quick_test`): Fast iteration (5 epochs, small batch)
2. **Full Training** (`full_training`): Production settings (100 epochs, augmentation)
3. **Grid Search** (`grid_search`): Comprehensive hyperparameter search

## ğŸ”¬ Hyperparameter Optimization

### Grid Search Parameters

The system supports comprehensive hyperparameter optimization:

- **Batch Size Factors**: `[0.5, 1.0, 2.0]` (multiplied by base batch size)
- **Learning Rates**: `[7e-6, 7e-5, 7e-4]` (logarithmic scale)
- **Loss Functions**: `["mean_squared_error"]` (extensible)
- **Cross-Validation**: 3-fold with stratification options

### Running Grid Search

```bash
# Quick grid search (2 folds, 5 epochs)
python train_cv_gridsearch_config.py --preset quick_test

# Full grid search (3 folds, 50 epochs)
python train_cv_gridsearch_config.py --preset grid_search
```

### Results Analysis

```bash
# Analyze cross-validation results
python analyze_cv_results.py --results_dir cv_results/cv_gridsearch_20250624_120000

# Generates:
# - Learning rate vs MAE plots
# - Training time comparisons
# - Fold MAE distributions
# - Summary tables and reports
```

## ğŸ“Š Output Structure

### Standard Training Output
```
experiments/experiment_name_YYYYMMDD_HHMMSS/
â”œâ”€â”€ config.json                    # Experiment configuration
â”œâ”€â”€ best_model.h5                  # Best model weights
â”œâ”€â”€ final_model.h5                 # Final model weights
â”œâ”€â”€ training_history.csv           # Training metrics history
â”œâ”€â”€ summary_report.txt             # Experiment summary
â”œâ”€â”€ validation_predictions.csv     # Validation set predictions
â””â”€â”€ test_predictions.csv          # Test set predictions (if available)
```

### Cross-Validation Output
```
cv_results/cv_gridsearch_YYYYMMDD_HHMMSS/
â”œâ”€â”€ config.json                    # Grid search configuration
â”œâ”€â”€ grid_search_results.csv        # All parameter combinations results
â”œâ”€â”€ best_parameters.json           # Optimal hyperparameters
â”œâ”€â”€ best_config.json              # Ready-to-use config with best parameters
â”œâ”€â”€ best_config.yaml              # Same config in YAML format
â”œâ”€â”€ summary_report.txt             # Grid search summary
â”œâ”€â”€ analysis/                      # Generated visualizations
â”‚   â”œâ”€â”€ lr_vs_mae.png
â”‚   â”œâ”€â”€ training_times.png
â”‚   â””â”€â”€ results_summary.csv
â””â”€â”€ params_X/                      # Individual parameter sets
    â”œâ”€â”€ parameters.json
    â””â”€â”€ fold_Y/
        â”œâ”€â”€ best_model.h5
        â””â”€â”€ history.csv
```

#### Reusing Best Configuration
After grid search, use the automatically generated best configuration:
```bash
# Train with optimal parameters found by grid search
python train_with_config.py --config cv_results/cv_gridsearch_20250624_120000/best_config.json
```

## ğŸ§  Model Details

### DBN-VGG16 Architecture

The model uses a sophisticated Deep Belief Network + VGG16 hybrid:

1. **VGG16 Backbone**: Pre-trained feature extraction
2. **Global Average Pooling**: Reduces spatial dimensions
3. **Dense Layers**: 
   - `dense_3`: 1024 units (ReLU activation)
   - `dropout_2`: 50% dropout for regularization
   - `dense_4`: 1 unit (linear output for age regression)

### Finetuning Strategy

- **Frozen Layers**: VGG16 convolutional layers (optionally unfrozen)
- **Trainable Layers**: Last 3 dense layers + optional VGG blocks
- **Learning Rate**: Reduced for stable finetuning (default: 1e-4)
- **Optimization**: Adam optimizer with learning rate scheduling

## ğŸ“ˆ Performance Monitoring

### Training Metrics
- **Loss**: Mean Squared Error (MSE)
- **Primary Metric**: Mean Absolute Error (MAE) in years
- **Validation**: Early stopping on validation MAE

### Callbacks
- **Model Checkpointing**: Save best model based on validation MAE
- **Early Stopping**: Prevent overfitting (patience: 10 epochs)
- **Learning Rate Reduction**: Adaptive LR scheduling
- **CSV Logging**: Detailed training history

## ğŸ–¥ï¸ HiPerGator Integration

### Default Paths (HiPerGator)
- **Images**: `/blue/cruzalmeida/pvaldeshernandez/slices_for_deepbrainnet_new`
- **Dataset**: `data/dataset_splits.csv`
- **Model**: `model/DBN_VGG16.h5`

### Batch Job Example
```bash
#!/bin/bash
#SBATCH --job-name=brain_age_cnn
#SBATCH --time=24:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=32gb
#SBATCH --gres=gpu:1

module load tensorflow/2.8.0

# Full training run
python train_with_config.py --preset full_training

# Grid search
python train_cv_gridsearch_config.py --preset grid_search
```

## ğŸ”§ Advanced Usage

### Custom Data Pipeline
```python
from data_loader_csv import create_sample_csv, get_keras_datasets
from config import ExperimentConfig

# Create custom dataset CSV
create_sample_csv("data/my_dataset.csv", img_dir="/path/to/images")

# Load data with custom configuration
config = ExperimentConfig()
config.data.image_list_csv = "data/my_dataset.csv"
train_data, val_data, test_data = get_keras_datasets(config)
```

### Model Evaluation
```python
# Detailed evaluation with ground truth
python inference.py --model_path model.h5 \
                   --batch_dir images/ \
                   --output_csv predictions.csv \
                   --truth_csv ground_truth.csv

# Generates MAE, RMSE, and detailed error analysis
```

## ğŸ“š Dependencies

### Core Requirements
- **TensorFlow** â‰¥ 2.6.0: Deep learning framework
- **NumPy** â‰¥ 1.19.0: Numerical computing
- **Pandas** â‰¥ 1.3.0: Data manipulation
- **Pillow** â‰¥ 8.0.0: Image processing
- **scikit-learn** â‰¥ 0.24.0: Machine learning utilities

### Visualization & Analysis
- **Matplotlib** â‰¥ 3.3.0: Plotting
- **Seaborn** â‰¥ 0.11.0: Statistical visualization
- **PyYAML** â‰¥ 5.4.0: YAML configuration support

## ğŸ¤ Contributing

### Code Style
- Follow PEP 8 style guidelines
- Use comprehensive docstrings for all functions
- Include type hints where appropriate
- Add unit tests for new functionality

### Adding New Features
1. Create feature branch
2. Implement with proper documentation
3. Add configuration options if needed
4. Update relevant documentation
5. Submit pull request

## ğŸ“„ License

This project is developed for academic research purposes. Please cite appropriately if used in publications.

## ğŸ†˜ Support

### Common Issues
1. **File Not Found**: Ensure all paths in configuration are correct
2. **Memory Errors**: Reduce batch size in configuration
3. **GPU Issues**: Check CUDA compatibility and memory usage

### Getting Help
- Check configuration validation warnings
- Review training logs in experiment directories
- Examine error messages in detail
- Verify data format matches expected CSV structure

---

**Author**: Developed for brain age prediction research  
**Last Updated**: December 2024  
**Version**: 2.0